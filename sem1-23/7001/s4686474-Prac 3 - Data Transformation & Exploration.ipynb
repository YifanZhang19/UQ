{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "<strong> This Prac is assessed, and the tasks are based on the demos/content from labs / lecture / tutorial and their materials. </strong>\n",
    "\n",
    "Complete the tasks given. \n",
    "\n",
    "* <strong>Make sure the R script submitted has no syntactical error, in which case a zero will be awarded for this Prac. Tutors will direct you on how to identify syntactical errors in your script.</strong>\n",
    "\n",
    "Additional Note\n",
    "* Variable names and strings are case-sensitive in R\n",
    "* Use of any packages to achieve the objective is strictly prohibited - unless explicitly mentioned in the question to do so\n",
    "* Any updates regarding the Pracs will be posted on <strong>[Ed discussion](https://edstem.org/au/courses/10549/discussion/)</strong>\n",
    "* You may find it helpful to read through the whole notebook and learn from the examples first, before solving the problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission [After Prac 5]\n",
    "\n",
    "This notebook, along with the \"Prac4\" and \"Prac5\" notebook will make up your \"Practical Assessment 2\" task which is worth 10% and is due **at 4pm on Friday May 5th (end of Week 10).** \n",
    "\n",
    "**Please refer to the submission guide on the LMS (Blackboard) for specific instructions.**\n",
    "\n",
    "* If you are not working within the UQ zones, please ensure you upload your work to your jupyter notebook inside the UQ zone using the same folder as that of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Data Transformation\n",
    "\n",
    "In computer programming, a *data type* of a variable determines what type of values it can contain and what operations can be performed. The following table summarises some of the common data types we will come across in R:\n",
    "\n",
    "| Data Type | Example                     | R Code                      |\n",
    "|-----------|-----------------------------|-----------------------------|\n",
    "| Logical   | TRUE, FALSE                 | `v <- TRUE`                 |\n",
    "| Numeric   | 12.3, 5, 999                | `v <- 23.5`                 |\n",
    "| Integer   | 1L, 34L, 0L                 | `v <- 2L`                   |\n",
    "| Character | \"a\", \"good\", \"TRUE\", '23.4' | `v <- \"TRUE\"`               |\n",
    "\n",
    "You can use the `class(var)` syntax to determine what object type a variable is, as demonstrated in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_var <- \"12345\" \n",
    "num_var <- 12345 \n",
    "class(char_var)\n",
    "class(num_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a dataset that contains inconsistent *data types* is a common data cleaning problem. The above example demonstrates two different ways the number 12345 could be expressed in a dataset, as a `character` or a `numeric` value. The data type of the variable determines what sort of operations can be performed on them.\n",
    "\n",
    "Datasets that are interpreted as the wrong data type, or that are *inconsistent* need to be cleaned so that the desired operations can be performed on the dataset. If a column that is supposed to contain integers contains characters, for example, we can no longer run numeric functions such as `mean()` on those values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a <- c(15, 20, 1, 10)\n",
    "b <- c(\"15\", \"1\", \"4\")\n",
    "\n",
    "# Valid! We can compute the mean of numberic values.\n",
    "print(mean(a))\n",
    "# You can't compute the mean from a set of characters by a number.\n",
    "print(mean(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, R contains built-in functions designed to convert between data-types. If we use the `as.numeric()` function, we can attempt to convert a `character` to a `numeric type`. Let's try the above example again, making sure we convert the characters to a numeric value before attempting to run the `mean` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b <- c(\"15\", \"1\", \"4\")\n",
    "mean(as.numeric(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting between data-types is a common feature of *data-cleaning* and *transformation*. Let's demonstrate by cleaning an example dataset.\n",
    "\n",
    "This dataset is a modified version of food-borne gastrointestinal illness in the US in 1940. The data has been modified from the original to include Brisbane-based addresses and some more obvious data integrity issues have been injected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(readr)\n",
    "oswego <- read_csv(\"OswegoTutorial.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step might be to explore the data and get a feel for the type of data we are working with. We can use the `head()` function to have a look at the first few rows of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(oswego)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to determine how large the dataset is, we can use the `dim()` function to determine the dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(oswego)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at the \"age\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oswego$age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice anything odd about this row? It looks like there was a data input error, and the number 7 has been inserted as the word \"seven\". Let's see how this affects our data analysis by trying to run the mean() function on the age column to determine the average age of people in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(oswego$age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we get an error, saying that the \"argument is not numeric or logical\". It looks like we can only run the mean function on a column that is *numeric* or *logical*. What data type is the age column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class(oswego$age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that R has interpreted this column as the 'character' data type, which explains why we can't run the mean function on it. Let's first replace the character(s) \"seven\" with \"7\" so that we can easily convert the whole column to a numeric data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oswego$age[oswego$age == \"seven\"] <- \"7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the above query a little bit. The `oswego$age == \"seven\"` is what is called a *conditional statement*, it matches values in `oswego$age` according to a specific condition. In this case, we match any of the rows that have the value \"seven\". We then set any of these rows to the character \"7\" instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 1.1</center>|\n",
    "| ---- |\n",
    "| Convert the **age** column to the correct data-type (**numeric**). <br> Store the **mean** of the ages in \"**meanAge1**\" variable |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "meanAge1 <- mean(as.numeric(oswego$age), na.rm = TRUE)\n",
    "meanAge1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that in our `oswego` dataset, we have the full address of each patient. Can you think of how this may be useful?\n",
    "\n",
    "One use of this locational data might be to see if outbreaks are clustered around particular suburbs/areas. In this case, being able to query the postcode directly would be useful, but currently the postcode is within the 'Address' column, which has the format:\n",
    "\n",
    "```\n",
    "38 Jones Road, South Brisbane, QLD 4101\n",
    "```\n",
    "\n",
    "One common aspect of preparing your data for use is making sure that it is in a format that is as simple to query as possible. For example, in the above case, if we wanted to find out what the most common suburb is which contained an outbreak, we would not be able to easily query suburb specifically with the `address` column as it contains a lot of superflous information.\n",
    "\n",
    "One solution might be to transform the data so each part of the address is in its own column - that way we query against a much simpler attribute such as postcode.\n",
    "\n",
    "Let's demonstrate this by splitting up the \"onsetdate\" column first as an example. You can see that the `onsetdate` is in the format:\n",
    "\n",
    "```\n",
    "19-Apr\n",
    "```\n",
    "If we wanted to query directly by month, we could separate the \"Month\" part of the address directly into its own column. Let's do that. We will use the \"tidyr\" library in R, a popular data transformation library. Run the code below and try and understand what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "library(tidyr)\n",
    "oswego_new <- separate(oswego, onsetdate, into = c(\"onset_day\", \"onset_month\"), sep = \"-\")\n",
    "head(oswego_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 1.2</center>|\n",
    "| ---- |\n",
    "| There is a **warning** message for 1 row (**row 24**) in the above cell. Indeed, there is an issue within the \"oswego\" dataframe if you examine the content of that row. <br> Write your R code to **fix** the issue (data/row) in \"**oswego**\" dataframe, and also **re-create** the \"**oswego_new**\" dataframe. <br> Note: You have to modify \"oswego\" dataframe, and then use `separate` function on modified \"oswego\" dataframe to create the \"oswego_new\" dataframe as done above. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "oswego$onsetdate[24] <- \"18-4\"\n",
    "oswego_new <- separate(oswego, onsetdate, into = c(\"onset_day\", \"onset_month\"), sep = \"-\")\n",
    "oswego_new$onset_month[oswego_new$onset_month == \"4\"] <- \"Apr\"\n",
    "oswego_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've separated the columns, we can directly query \"month\" to see the range of months that these outbreaks occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique(oswego_new$onset_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this data is limited to April, June and also that a lot of the onset date data is missing, resulting in 'NA' values. Now let's see how we can get useful information from the location information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 1.3</center>|\n",
    "| ---- |\n",
    "| **Separate** the \"**address**\" column of \"**oswego**\" dataframe into **four** new columns, `address`, `suburb_name`, `state` and `postcode` and store the result inside \"**oswego_address_separate**\" dataframe |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "library(tidyr)\n",
    "oswego_address_separate <- separate(oswego, address, into = c(\"address\", \"suburb_name\", \"state\", \"postcode\"), sep = \",\")\n",
    "oswego_address_separate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 1.4</center>|\n",
    "| ---- |\n",
    "| Which **postcode** has the **most** **occurrences** of **illness**? Remember, you will need to **filter** your data by **only those who are ill**. <br> Store the result inside \"**mostIll1**\" variable. |\n",
    "|Hint: you can use `table` function to count the occurrences and use `names` to return those postcodes|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "ill <- subset(oswego_address_separate, ill=='yes')\n",
    "postcode <- table(ill$postcode)\n",
    "postcode_count <- max(postcode)\n",
    "mostIll1 <- names(postcode)[which(postcode==postcode_count)]\n",
    "mostIll1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>Extension TASK</center>|\n",
    "| ---- |\n",
    "| List **all postcodes and number** of occurences using (1) **R** or (2) **SQL** (tip: use group by query). **This question will not be graded**.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "\"select postcode, count(*) as number from table group by postcode order by postcode\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 1.5</center>|\n",
    "| ---- |\n",
    "| Calculate the **mean age** for each **postcode**, with `NA` **excluded**, and store the result inside \"**meanAgePost1**\" dataframe with column names (**PostCode, MeanAge**). <br> Is there **any anomaly** in the result? If yes, then write down the cause of the anomaly in \"anomalyComment1\" variable (as a string). <br> If there is **an anomaly**, then store the postcodes that has anomaly inside \"anomaly1\" vector. <br> If there is **no anomaly**, then store `NA` inside \"anomaly1\" and \"anomalyComment1\" variables. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "oswego_address_separate <- oswego_address_separate[complete.cases(oswego_address_separate$age),]\n",
    "oswego_address_separate$age <- as.numeric(oswego_address_separate$age)\n",
    "meanAgePost1 <- aggregate(age ~ postcode, oswego_address_separate, FUN = function(x) mean(x, na.rm = TRUE))\n",
    "colnames(meanAgePost1) <- c(\"PostCode\", \"MeanAge\")\n",
    "meanAgePost1   \n",
    "anomaly1 <- NA\n",
    "anomalyComment1 <- NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Data Exploration\n",
    "\n",
    "In Prac 1, we already encountered the HR Analytics dataset.\n",
    "\n",
    "The question we now seek to answer is: ***Why are our best and most experienced employees leaving?***\n",
    "\n",
    "To get to grips with the data, we will carry out some **exploratory data analysis (EDA)** techniques in R.\n",
    "\n",
    "Firstly, let's import the data and look at a few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(readr)\n",
    "HR_comma_sep <- read_csv(\"HR_comma_sep.csv\")\n",
    "HR_comma_sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 2.1</center>|\n",
    "| ---- |\n",
    "| What type of variable is `left`? Here \"type\" refers to the variable types mentioned in the lecture. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]. Include the desciptive portion of the answer as an R comment like this line.\n",
    "t <- class(HR_comma_sep$left)\n",
    "t\n",
    "#\"double\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's count the number of rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(!complete.cases(HR_comma_sep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no missing data, we can proceed with our analysis on the complete data set.\n",
    "\n",
    "Let's explore some simple quantitative summaries.  The default summary statistics are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(HR_comma_sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These statistics tell us a bit about each variable in isolation; what we would like to do is obtain statistics pertinent to our question.\n",
    "\n",
    "To proceed, it is useful to classify each variable as either a *response* or a *predictor*.  For this problem and data set, it is clear that `left` is the response and all other variables are predictors.\n",
    "\n",
    "For example: What is the breakdown of `left` by job Department (`sales`)?\n",
    "\n",
    "This is easily achieved by creating a *two-way contingency* table, which counts the number of intances in each `left` by `sales` cell in a two by two table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table<-table(HR_comma_sep$sales,HR_comma_sep$left)\n",
    "test_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginal counts and proportions are easily created as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin.table(test_table) \n",
    "margin.table(test_table,1)\n",
    "margin.table(test_table,2)\n",
    "\n",
    "prop.table(test_table)\n",
    "prop.table(test_table,1)\n",
    "prop.table(test_table,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These proportions can also be conveniently visualised using a mosaic plot, where the widths of rectangles are proportional to the number of observations in the x variable categories (`Department`), and, for each x variable category, the heights are proportional to the number of observations in the corresponding y variable categories (`Left`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaicplot(test_table,main=\"Mosaic Plot of Left by Department\",xlab=\"Department\",ylab=\"Left\",las=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 2.2</center>|\n",
    "| ---- |\n",
    "| Which **Department** has the **highest** proportion of employees that have **left**, relative to that department?  Which Department has the **lowest**? <br> Store the answer inside \"**highest1**\" and \"**lowest1**\" R variables respectively. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "left_prop <- prop.table(test_table,1)\n",
    "highest1 <- rownames(left_prop)[which.max(left_prop[, 2])]\n",
    "highest1\n",
    "lowest1  <- rownames(left_prop)[which.min(left_prop[, 2])]\n",
    "lowest1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the breakdown of average hours worked per month by left?\n",
    "\n",
    "We will first create visual summaries using boxplots (which display summary statistics visually)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(HR_comma_sep$average_montly_hours~HR_comma_sep$left,xlab=\"Left\",ylab=\"Average Monthly Hours\",main=\"Boxplot of Average Monthly Hours by Left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple visual summary suggests that employees that left typically worked longer hours.\n",
    "\n",
    "We can get a more detailed view by constructing a histogram as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(HR_comma_sep$average_montly_hours[HR_comma_sep$left==1], col=rgb(1,0,0,0.5),main=\"Histogram for Average Monthly Hours by Left\", xlab=\"Average Monthly Hours\")\n",
    "hist(HR_comma_sep$average_montly_hours[HR_comma_sep$left==0], col=rgb(0,0,1,0.5),add=TRUE)\n",
    "legend(\"topright\", c(\"Not Left\",\"Left\"),fill=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram reveals an interesting feature of the data; the distribution of average monthly hours for employees that left is *multimodal*.  This suggests employees that leave fall into two groups: those that work lower than normal hours, and those that work long hours.\n",
    "\n",
    "The same information can be seen from a plot of the empirical cumulative distribution function (ecdf) for average monthly hours by left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E0<-ecdf(HR_comma_sep$average_montly_hours[HR_comma_sep$left==0])\n",
    "E1<-ecdf(HR_comma_sep$average_montly_hours[HR_comma_sep$left==1])\n",
    "plot(E0,col=rgb(0,0,1,0.5),verticals = TRUE, do.points = FALSE,main=\"ECDF for Average Monthly Hours by Left\", xlab=\"Average Monthly Hours\")\n",
    "plot(E1,col=rgb(1,0,0,0.5),verticals = TRUE, do.points = FALSE,add=TRUE)\n",
    "legend(\"bottomright\", c(\"Not Left\",\"Left\"),lwd=1, lty=c(1,1),col=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the time spent in the company by left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(2,1)) # Create 2 by 1 figure\n",
    "# Plot Frequencies\n",
    "hist(HR_comma_sep$time_spend_company[HR_comma_sep$left==0],breaks=c(0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5),col=rgb(0,0,1,0.5),main=\"Histogram for Time Spent at Company by Left\", xlab=\"Time Spent at Company (Years)\",freq=TRUE)\n",
    "hist(HR_comma_sep$time_spend_company[HR_comma_sep$left==1],breaks=c(0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5),col=rgb(1,0,0,0.5),add=TRUE,freq=TRUE)\n",
    "legend(\"topright\", c(\"Not Left\",\"Left\"),fill=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)))\n",
    "\n",
    "# Plot Proportions\n",
    "hist(HR_comma_sep$time_spend_company[HR_comma_sep$left==1],breaks=c(0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5),col=rgb(1,0,0,0.5),main=\"Histogram for Time Spent at Company by Left\", xlab=\"Time Spent at Company (Years)\",freq=FALSE)\n",
    "hist(HR_comma_sep$time_spend_company[HR_comma_sep$left==0],breaks=c(0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5),col=rgb(0,0,1,0.5),add=TRUE,freq=FALSE)\n",
    "legend(\"topright\", c(\"Not Left\",\"Left\"),fill=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the above plots that the densities (`Density`) are easier than the frequency counts to visually compare, although relative magnitude information is lost in the process.\n",
    "\n",
    "The shape of the distributions for time spent at the company are noticably different, with employees that have been at the company for very short of very long periods of time being less likely to leave.  Most employees that leave the company have worked there for between three and five years, inclusive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 2.3</center>|\n",
    "| ---- |\n",
    "| Construct a **histogram** for **last evaluation by left**.  What is a **possible explanation** for any patterns you see? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]. Include the desciptive portion of the answer as an R comment.\n",
    "hist(HR_comma_sep$last_evaluation[HR_comma_sep$left==1], col=rgb(1,0,0,0.5),main=\"Histogram for last evaluation by Left\", xlab=\"last evaluation\",freq=FALSE)\n",
    "hist(HR_comma_sep$last_evaluation[HR_comma_sep$left==0], col=rgb(0,0,1,0.5),add=TRUE, freq=FALSE)\n",
    "legend(\"topright\", c(\"Not Left\",\"Left\"),fill=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)))\n",
    "# not leave employees have more even evaluation, and leave employees have more extreme evaluaion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 2.4</center>|\n",
    "| ---- |\n",
    "| Construct **density plots** to compare the **average monthly working hours**, **satisfaction level**, and **last evaluation for those who left management and HR**. Are the patterns similar or different for these two departments?|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]. Include the desciptive portion of the answer as an R comment.\n",
    "left_people <- HR_comma_sep[HR_comma_sep$left == 1,]\n",
    "hist(left_people$average_montly_hours[left_people$sales==\"management\"],col=rgb(1,0,0,0.5),main=\"Histogram for average montly hours by Left\", xlab=\"average montly hours\",freq=FALSE)\n",
    "hist(left_people$average_montly_hours[left_people$sales==\"hr\"],col=rgb(0,0,1,0.5),add=TRUE, freq=FALSE)\n",
    "legend(\"topright\", c(\"hr\",\"management\"),fill=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)))\n",
    "\n",
    "hist(left_people$satisfaction_level[left_people$sales==\"management\"],col=rgb(1,0,0,0.5),main=\"Histogram for satisfaction by Left\", xlab=\"satisfaction\",freq=FALSE)\n",
    "hist(left_people$satisfaction_level[left_people$sales==\"hr\"],col=rgb(0,0,1,0.5),add=TRUE, freq=FALSE)\n",
    "legend(\"topright\", c(\"hr\",\"management\"),fill=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)))\n",
    "\n",
    "hist(left_people$last_evaluation[left_people$sales==\"management\"],col=rgb(1,0,0,0.5),main=\"Histogram for last evaluation by Left\", xlab=\"last evaluation\",freq=FALSE)\n",
    "hist(left_people$last_evaluation[left_people$sales==\"hr\"],col=rgb(0,0,1,0.5),add=TRUE, freq=FALSE)\n",
    "legend(\"topright\", c(\"hr\",\"management\"),fill=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5))) \n",
    "\n",
    "# in this three different things, this two part are not silmilar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wish to plot last evaluation and average monthly hours by leave.  One way to do this is with a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(HR_comma_sep$average_montly_hours[HR_comma_sep$left==0],HR_comma_sep$last_evaluation[HR_comma_sep$left==0],main=\"Scatter Plot for Average Monthly Hours vs \\n Last Evalation by Left\", xlab=\"Average Monthly Hours\",ylab=\"Last Evaluation\",col=rgb(0,0,1,0.5))\n",
    "points(HR_comma_sep$average_montly_hours[HR_comma_sep$left==1],HR_comma_sep$last_evaluation[HR_comma_sep$left==1],col=rgb(1,0,0,0.5))\n",
    "legend(\"bottomright\", c(\"Not Left\",\"Left\"),pch=c(1,1),col=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plot, we gain additional insight into the relationship between last evaluation, average monthly hours, and those that left.  The bulk of those leaving that worked normal hours also received low evaluations, and most of those leaving that worked long hours received high evaluations.\n",
    "\n",
    "We can break down the scatterplot further by an additional variable with the construction of a scatter plot matrix.  For instance, we might be curious how satisfaction level, last evaluation, average monthly hours, and left relate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"lattice\") #Load the `Lattice' graphics package\n",
    "HR_subset<-subset(as.data.frame(HR_comma_sep),select=c('average_montly_hours', 'last_evaluation', 'satisfaction_level', 'left'))\n",
    "super.sym <- trellis.par.get(\"superpose.symbol\") #Get Symbol Plotting Information\n",
    "splom(~HR_subset[1:3],groups=left,data = HR_subset,varnames=c(\"Average \\nMonthly \\nHours\",\"Last \\nEvaluation\",\"Satisfaction \\nLevel\"),\n",
    "      key = list(title = \"Scatterplot Matrix\",\n",
    "                 columns = 2, \n",
    "                 points = list(pch = super.sym$pch[1:2],\n",
    "                 col = super.sym$col[1:2]),\n",
    "                 text = list(c(\"Not Left\", \"Left\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code produces a very high quality scatter plot, but has a lot of parameters to set. In practice, we typically want to get a quick and dirty (but working) plot first. This can be achieved using a one-liner `splom(~HR_subset[1:3], groups=left, data=HR_subset)`, or an even simpler one-liner `splom(~HR_subset[1:3], groups=HR_subset$left)`. Try these out. Also try this one-liner: `pairs(HR_subset[,1:3], col=as.factor(HR_subset$left))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 2.5</center>|\n",
    "| ---- |\n",
    "| We have demonstrated the use of a few tools for visually exploring the relationships between two variables, including **mosaic plot**, **boxplot**, **histogram**, **ECDF plot**, **scatter plots**. Write code to apply these tools (if applicable) to **visually explore** the **relationship between of the number of projects and leaving**. What is a possible explanation for any patterns you see? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here].  Include the desciptive portion of the answer as an R comment.\n",
    "hist(HR_comma_sep$number_project[HR_comma_sep$left==1],breaks=c(2,3,4,5,6,7,8),col=rgb(1,0,0,0.5),main=\"Histogram for number of projcts who left\", xlab=\"last evaluation\")\n",
    "# the most left employees have 2 projects.\n",
    "plot(HR_comma_sep$number_project[HR_comma_sep$left==1],main=\"Scatter Plot for number of projcts who left\", xlab=\"Number of left employees\",ylab=\"Number of projects\",col=rgb(0,0,1,0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have been exploring the entire data set.  Let us return to the original question: *Why are our best and most experienced employees leaving?*\n",
    "\n",
    "To get to grips with this, we need to identify which subset of employees are \"best\" and \"most experienced\".\n",
    "Precisely what this means to any particular person is ambiguous.  When encountering ambiguity in the problem, the process of resolving that ambiguity involves a two-way dialogue with the *problem poser*.\n",
    "\n",
    "Broadly, one might imagine this subset to contain:\n",
    "\n",
    "- Employees with high evaluations.\n",
    "- Employees the have been with the company for a while.\n",
    "\n",
    "Additional criteria might be:\n",
    "\n",
    "- Employees that work on a large number of projects.\n",
    "- Employees that work a lot.\n",
    "\n",
    "For now, suppose that the \"best\" employees are those with an evaluation of 0.8 or higher, and the \"most experienced\" employees are those that have been with the company for 4 or more years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 2.6</center>|\n",
    "| ---- |\n",
    "| Create a **subset** of the \"**best**\" and \"**most experienced**\" **employees** by appropriately filtering the entire data set. Store the result inside \"**bestExp1**\" dataframe. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "bestExp1 <- HR_comma_sep[HR_comma_sep$last_evaluation >= 0.8 & HR_comma_sep$time_spend_company >= 4,]\n",
    "bestExp1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 2.7</center>|\n",
    "| ---- |\n",
    "| Perform **EDA** on the **subset** of \"best\" and \"most experienced\" employees you **just created**.  What is a possible explanation for any patterns you see? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]. Include the desciptive portion of the answer as an R comment.\n",
    "hist(bestExp1$satisfaction_level[bestExp1$left==1],col=rgb(1,0,0,0.5),main=\"Histogram for satisfaction_level for best and most experienced employees \", xlab=\"satisfaction_level\",freq=FALSE)\n",
    "hist(bestExp1$satisfaction_level[bestExp1$left==0],col=rgb(0,0,1,0.5),add=TRUE, freq=FALSE)\n",
    "legend(\"topright\", c('left','not left'),fill=c(rgb(1,0,0,0.5), rgb(0,0,1,0.5)))\n",
    "# the left employees have low satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Unusual Data\n",
    "\n",
    "Codifying which observations are \"unusual\" goes hand-in-hand with the statistical model for data; in particular the assumptions we make about the distribution of the residuals.\n",
    "\n",
    "A common assumption in statistical analyses is that the residuals follow a *normal distribution*.\n",
    "\n",
    "Let's simulate 200 normally distributed observations, synthesising heights (in cm) of adult men, plot a histogram, and display standard summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_data<-rnorm(200, mean = 174.46, sd = 7.15) \n",
    "hist(height_data,col=rgb(0,0,1,0.5),main=\"Histogram for Synthetic Height Data\", xlab=\"Height (cm)\")\n",
    "summary(height_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice the symmetric unimodal shape of the histogram.  Re-run the code above several times and observe how the distribution of the data retains these characteristics.\n",
    "\n",
    "Next, we will create a copy of the data, artificially convert the first 10 observations from units of cm to units of inches, plot another histogram, and display standard summary statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_data_out<-height_data\n",
    "height_data_out[1:10]<-height_data_out[1:10]*0.3937007874\n",
    "hist(height_data_out,col=rgb(0,0,1,0.5),main=\"Histogram for Synthetic Height Data (with Unusual Obs.)\", xlab=\"Height (cm)\")\n",
    "summary(height_data_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this artifical data, it is immediately clear from the histogram that there are unusual observations, widely separated from the bulk.\n",
    "\n",
    "Despite both being measures of central tendency, you will notice that the `mean` values have changed noticably, but the `median` values have barely moved.  This is because the `mean` is a sensitive statistic (and consequently is greatly impacted by unusual observations; influential observations in this case).  On the other hand the `median` is a rather insensitive (a *robust statistic*, and is hardly impacted by unusual observations).\n",
    "\n",
    "Using simple robust measures of central tendency (median) and variability [interquartile range (IQR)], the boxplot visually flags observations that lie outside $\\textrm{Median} \\pm 1.5 \\times \\textrm{IQR}$.  These are the observations that may depart from the underlying assumption of normality; the outliers.\n",
    "\n",
    "Let's create and store a boxplot of the height data with unusual observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdo_box<-boxplot(height_data_out,main=\"Boxplot of Synthetic Height Data (with Unusual Obs.)\", ylab=\"Height (cm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our artifical unusual observations are clearly labelled as outliers.  \n",
    "\n",
    "We can extract them from the stored boxplot as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdo_box$out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this instance, after examining the outliers and the context of the data set, it is clear that the reason for these outliers is a simple unit change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Data Imputation\n",
    "\n",
    "Let's load up Karl Pearsons' data on the heights (in inches) of fathers and their sons, produce a scatter plot of the data, and display standard summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"readr\")\n",
    "fheight<-read_csv(\"./PearsonFather.csv\",col_names=\"fheight\",col_types=\"d\")\n",
    "sheight<-read_csv(\"./PearsonSon.csv\",col_names=\"sheight\",col_types=\"d\")\n",
    "fs_height<-data.frame(fheight,sheight)\n",
    "plot(fs_height$fheight,fs_height$sheight,main=\"Father and Son Heights\", xlab=\"Father Heights (in)\",ylab=\"Son Heights (in)\",col=rgb(1,0.3,0.1,0.5))\n",
    "summary(fs_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a version of the data set with both father and son heights missing completely at random (MCAR), plot the data, and display summary statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fheight_MCAR<-fheight\n",
    "set.seed(55)\n",
    "fheight_MCAR[rbinom(nrow(fheight),1,0.1)==1,]<-NA\n",
    "sheight_MCAR<-sheight\n",
    "set.seed(55)\n",
    "sheight_MCAR[rbinom(nrow(sheight),1,0.1)==1,]<-NA\n",
    "fs_height_MCAR<-data.frame(fheight_MCAR,sheight_MCAR)\n",
    "plot(fs_height_MCAR$fheight,fs_height_MCAR$sheight,main=\"Father and Son Heights (MCAR)\", xlab=\"Father Heights (in)\",ylab=\"Son Heights (in)\",col=rgb(1,0.3,0.1,0.5))\n",
    "summary(fs_height_MCAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets examine the effect of some simple imputation strategies, using the `mice` package.\n",
    "\n",
    "First up is mean imputation (**not recommended ever in practice**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install.packages(\"mice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"mice\")\n",
    "imp_mean<-mice(fs_height_MCAR,method=\"mean\",m=1,maxit=1)\n",
    "fsh_mean_imp<-complete(imp_mean,1)\n",
    "fsh_mean_imp$mis<-!complete.cases(fs_height_MCAR)\n",
    "plot(fsh_mean_imp$fheight[fsh_mean_imp$mis==FALSE],fsh_mean_imp$sheight[fsh_mean_imp$mis==FALSE],main=\"Father and Son Heights (MCAR)\", xlab=\"Father Heights (in)\",ylab=\"Son Heights (in)\",col=rgb(1,0.3,0.1,0.5))\n",
    "points(fsh_mean_imp$fheight[fsh_mean_imp$mis==TRUE],fsh_mean_imp$sheight[fsh_mean_imp$mis==TRUE],col=rgb(0.1,0.7,1,0.5))\n",
    "legend(\"bottomright\", c(\"Complete Cases\",\"Imputed Cases\"),pch=c(1,1),col=c(rgb(1,0.3,0.1,0.5), rgb(0.1,0.7,1,0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distortion to the data is clearly observable in the plot above.  This distortion is reflected in its effect on the standard deviations of the variables, as can be seen as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as.data.frame(lapply(fs_height,sd)) # Source Data Standard Deviations\n",
    "as.data.frame(lapply(fs_height_MCAR,sd,na.rm=TRUE)) # Complete-case MCAR Data Standard Deviations\n",
    "as.data.frame(lapply(fsh_mean_imp[1:2],sd,na.rm=TRUE)) # Mean-imputed MCAR Data Standard Deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a small change in commands, we can perform linear regression imputation (using `method=\"norm.nob\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_linreg<-mice(fs_height_MCAR,method=\"norm.nob\",m=1)\n",
    "fsh_linreg_imp<-complete(imp_linreg,1)\n",
    "fsh_linreg_imp$mis<-!complete.cases(fs_height_MCAR)\n",
    "plot(fsh_linreg_imp$fheight[fsh_linreg_imp$mis==FALSE],fsh_linreg_imp$sheight[fsh_linreg_imp$mis==FALSE],main=\"Father and Son Heights (MCAR)\", xlab=\"Father Heights (in)\",ylab=\"Son Heights (in)\",col=rgb(1,0.3,0.1,0.5))\n",
    "points(fsh_linreg_imp$fheight[fsh_linreg_imp$mis==TRUE],fsh_linreg_imp$sheight[fsh_linreg_imp$mis==TRUE],col=rgb(0.1,0.7,1,0.5))\n",
    "legend(\"bottomright\", c(\"Complete Cases\",\"Imputed Cases\"),pch=c(1,1),col=c(rgb(1,0.3,0.1,0.5), rgb(0.1,0.7,1,0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple imputation is requested by adjusting the `m=` argument; for example `m=10` will carry out the imputation process 10 times.\n",
    "\n",
    "As a final exercise, we will fit a linear regression model to (1) the original data; (2) the complete-case data; (3) data multiply-imputated with linear regression;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_all<-with(fs_height,lm(sheight~fheight))\n",
    "\n",
    "fit_MCAR<-with(fs_height_MCAR,lm(sheight~fheight))\n",
    "\n",
    "mimp_linreg<-mice(fs_height_MCAR,method=\"norm.nob\",m=10)\n",
    "fit_linreg_mimp<-with(mimp_linreg,lm(sheight~fheight))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression fits found using each approach are summarised as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(fit_all)\n",
    "summary(fit_MCAR)\n",
    "summary(pool(fit_linreg_mimp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other imputation methods include simple random imputation from observed data (`method=\"sample\"`).\n",
    "\n",
    "|<center>TASK 3.1</center>|\n",
    "| ---- |\n",
    "|Complete \"**fs_height_MCAR**\" dataset using **simple random imputation**. Use `mice` function for the same. <br> Set \"**Number of multiple imputations**\" in \"mice\" function to **1**. Set the `seed` argument in mice function to **55**. <br> Store the completed data frame (result of `complete` function) in \"**simpleRandomImp1**\" variable. <br> Comment on how the data imputed using this method compares to data imputed using linear regression. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]. Include the desciptive portion of the answer as an R comment.\n",
    "set.seed(55)\n",
    "simple_random <- mice(fs_height_MCAR, method = \"norm\", m = 1, maxit = 1)\n",
    "simpleRandomImp1 <- complete(simple_random, 1)\n",
    "#Simple random imputation use random data in from observed data and linear regression imputation use a linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension\n",
    "\n",
    "Although the focus in this course is on the more traditional tools of **EDA**, ***text mining*** and the associated **visualisation of word frequencies** through the use of ***word clouds*** can be a compelling way to present/explore textual data.\n",
    "\n",
    "In this extension, we will step through the basic process of reading in **text data**, **preparing text data for text mining**, **determining word frequencies**, and **visualising the resulting frequencies in a word cloud**.\n",
    "\n",
    "Let's start by loading a few packages and some text data.\n",
    "\n",
    "**Note:** The extension is not assessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"tm\")\n",
    "install.packages(\"wordcloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"tm\") # Text mining library\n",
    "library(\"wordcloud\") # Wordcloud plotting library\n",
    "ulysses_raw<-readLines(\"./pg1727.txt\")\n",
    "ulysses_raw<-ulysses_raw[342:10599] # Strip out front and back matter\n",
    "ulysses<-Corpus(VectorSource(ulysses_raw)) # Convert to corpus format\n",
    "inspect(ulysses[1:30]) # Inspect first 30 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process the corpus and extract frequently used words (excluding commonly used words known as *stop words*), we need to remove punctuation, numbers, case of type, stop words, and strip out any excess whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulysses <- tm_map(ulysses, content_transformer(tolower)) # Convert corpus to lower case\n",
    "ulysses <- tm_map(ulysses, removePunctuation) # Strip common punctuation\n",
    "ulysses <- tm_map(ulysses, removeNumbers) # Strip numbers \n",
    "ulysses <- tm_map(ulysses, removeWords, stopwords(\"english\")) # Strip default stop words\n",
    "\n",
    "# Strip custom stop words\n",
    "ulysses <- tm_map(ulysses, removeWords, c(\"i\",\"and\",\"are\",\"it\",\"ii\",\"iii\",\"iv\",\"v\",\"vi\",\"vii\",\"viii\",\"ix\",\"x\",\"xi\",\"xii\",\"xiii\",\"xiv\",\"xv\",\"xvi\",\"xvii\",\"xviii\",\"xix\",\"xx\",\"xxi\",\"xxii\",\"xxiii\",\"xxiv\",\"xxv\")) \n",
    "\n",
    "ulysses <- tm_map(ulysses, stripWhitespace) # Strip excess whitespace\n",
    "\n",
    "inspect(ulysses[1:30]) # Inspect first 30 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a *term document matrix*, which counts word occurence by corpus.  Then we keep only the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulysses_tdm <- TermDocumentMatrix(ulysses) # Create a table counting word occurences by corpus\n",
    "ulysses_tdm_sparse<-removeSparseTerms(ulysses_tdm , 0.995) # Keep only words that appear more than (1-0.995)*10258 (around 51) times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the sparse term document matrix, we will create word counts as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulysses_tdms_matrix <- as.matrix(ulysses_tdm_sparse) # Convert to matrix for processing\n",
    "ulysses_tdms_freqs <- sort(rowSums(ulysses_tdms_matrix),decreasing=TRUE) # Compute frequencies and sort\n",
    "ulysses_tdms_freqs_df <- data.frame(terms = names(ulysses_tdms_freqs),freq=ulysses_tdms_freqs) # Construct data frame\n",
    "head(ulysses_tdms_freqs_df, 5) # Inspect the top 5 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will plot the most common words in a *word cloud*, in which the size of the word is proportional to its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set.seed(8888)\n",
    "wordcloud(words = ulysses_tdms_freqs_df$terms, freq = ulysses_tdms_freqs_df$freq, min.freq = 1,max.words=100, random.order=FALSE, random.color=FALSE, rot.per=0.4,colors=brewer.pal(8, \"Dark2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This Line gets printed if there is no error, when Kernel -> Restart & Run All\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
