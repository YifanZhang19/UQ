{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "<strong> This Prac is assessed, and the tasks are based on the demos/content from labs / lecture / tutorial and their materials. </strong>\n",
    "\n",
    "Complete the tasks given. \n",
    "\n",
    "* <strong>Make sure the R script submitted has no syntactical error, in which case a zero will be awarded for this Prac. Tutors will direct you on how to identify syntactical errors in your script.</strong>\n",
    "\n",
    "Additional Note\n",
    "* Variable names and strings are case-sensitive in R\n",
    "* Use of any packages to achieve the objective is strictly prohibited - unless explicitly mentioned in the question to do so\n",
    "* Any updates regarding the Pracs will be posted on <strong>[Ed discussion](https://edstem.org/au/courses/10549/discussion/)</strong>\n",
    "* You may find it helpful to read through the whole notebook and learn from the examples first, before solving the problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission [After Prac 5]\n",
    "\n",
    "This notebook, along with the \"Prac3\" and \"Prac5\" notebook will make up your \"Practical Assessment 2\" task which is worth 10% and is due **at 4pm on Friday May 5th (end of Week 10).** \n",
    "\n",
    "**Please refer to the submission guide on the LMS (Blackboard) for specific instructions.**\n",
    "\n",
    "* If you are not working within the UQ zones, please ensure you upload your work to your jupyter notebook inside the UQ zone using the same folder as that of this notebook.\n",
    "\n",
    "**NOTE:** Prac 4 (this prac) has two notebooks for assessment; \n",
    " - Prac4 (Part 1) - Aggregate\n",
    " - Prac4 (Part 2) - Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Learning\n",
    "\n",
    "In this practical, you will be guided to apply what you learn about regression, classification and clustering to some datasets. We provide more detailed explanations for some of the concepts introduced in lecture, and introduce some new concepts. This notebook also introduces and illustrates some useful R libraries for regression, classification and clustering. You will find some problems that you need to solve throughout the notebook.\n",
    "\n",
    "You may find it helpful to read through the whole notebook and learn from the examples first, before solving the problems.\n",
    "\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Let's load up Karl Pearsons' data on the heights (in inches) of fathers and their sons, produce a scatter plot of the data, and display standard summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.new(width=5, height=4) # set the figure size to be 5in x 4in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"readr\")\n",
    "fheight<-read_csv(\"./PearsonFather.csv\",col_names=\"fheight\",col_types=\"d\")\n",
    "sheight<-read_csv(\"./PearsonSon.csv\",col_names=\"sheight\",col_types=\"d\")\n",
    "fs_height<-data.frame(fheight,sheight)\n",
    "plot(fs_height$fheight,fs_height$sheight,main=\"Father and Son Heights\", xlab=\"Father Heights (in)\",ylab=\"Son Heights (in)\",col=rgb(1,0.3,0.1,0.5))\n",
    "summary(fs_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting a linear model.**\n",
    "We will fit a linear regression model to this data using the built-in `lm` function, with Father Heights (in) as the predictor, and Son Heights (in) as the response. The following illustrates how this can be done. You can learn more about the function by running `help(lm)` to read the function's documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmfit<-with(fs_height,lm(sheight~fheight))\n",
    "summary(lmfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary information contains key information about the estimated model, including those that we covered in lecture.\n",
    "\n",
    "|<center>TASK 1</center>|\n",
    "| ---- |\n",
    "| From the linear regression model above, how much does the son's height vary when father's height increases by 1 unit (exclude the error term). <br> Store the answer inside \"fsCoef1\" R variable. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>fheight:</strong> 0.51400591254559"
      ],
      "text/latex": [
       "\\textbf{fheight:} 0.51400591254559"
      ],
      "text/markdown": [
       "**fheight:** 0.51400591254559"
      ],
      "text/plain": [
       "  fheight \n",
       "0.5140059 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Place your Answer here]\n",
    "fsCoef1 <- coef(lmfit)[2]\n",
    "fsCoef1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 2</center>|\n",
    "| ---- |\n",
    "| From the linear regression model above, what is the son's height when father's height is 0 (exclude the error term). <br> Store the answer inside \"intCoef1\" R variable. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>(Intercept):</strong> 33.8928005406617"
      ],
      "text/latex": [
       "\\textbf{(Intercept):} 33.8928005406617"
      ],
      "text/markdown": [
       "**(Intercept):** 33.8928005406617"
      ],
      "text/plain": [
       "(Intercept) \n",
       "    33.8928 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Place your Answer here]\n",
    "intCoef1 <- coef(lmfit)[1]\n",
    "intCoef1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking model assumptions.**\n",
    "Before taking the model seriously, we need to check whether the assumptions on our error model are satisfied. \n",
    "\n",
    "To do so, we need to explore the model's *residuals*, i.e., the differences between the actual values and the predicted values, or more precisely, the values of $y - \\beta_{0} - \\beta_{1} x$ for a simple linear model.\n",
    "\n",
    "The assumption that we make on the residuals can be broken down into three assumptions\n",
    "(1) zero mean; (2) constant variance, or homoscedasticity; and (3) normality.\n",
    "\n",
    "Let's first extract the residuals, plot them, and compute summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmfit.res<-resid(lmfit)\n",
    "plot(fs_height$fheight,lmfit.res,\n",
    "     main=\"Residual Son Heights from Simple Linear Regression\", \n",
    "     xlab=\"Father Heights (in)\",\n",
    "     ylab=\"Residual Son Heights (in)\",\n",
    "     col=rgb(1,0.3,0.1,0.5))\n",
    "summary(lmfit.res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By examining the plot above, Assumption (1) seems reasonable.  Assumption (2) seems to hold for Father Heights in the range from about 63 in to 71 in.  Outside this range, there may be some *heteroscedasticity* (change in the variability).  However, it is relatively mild, and we will proceed as though this assumption is satistfied.\n",
    "\n",
    "To check Assumption (3), we will create a quantile-quantile (Q-Q) plot of the empirical distribution of the residuals against the standard normal distribution. A Q-Q plot for two distributions is a plot of the quantiles of one distribution against those of another. From a practical point of view, it is sufficient to know that if the residuals come from a normal distribution, the points in the Q-Q plot should approximately lie on a straight line. For those interested in better understanding the Q-Q plot, the Wikipedia article on Q-Q plot is a good reference: https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot.\n",
    "\n",
    "A Q-Q plot can be conveniently generated in R using the `qqPlot` function from the `car` library. It allows us to generate the Q-Q plot together with a straight-line fit and pointwise 95% confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"car\")\n",
    "qqPlot(lmfit.res,xlab=\"Standard Normal Quantiles\",ylab=\"Quantiles of the Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot, the bulk of the residual quantiles appear to follow a normal distribution, with slightly heavier left and right *tails*, suggesting a mild departure from the Assumption (3).\n",
    "\n",
    "Besides using the Q-Q plot as a graphical test for Assumption (3), we can also formally test it by carrying out a Shapiro-Wilk test. The Shapiro-Wilk test computes a summary statistic $W$ from the residuals, and then computes the  whether probability (p-value) of observing the value of $W$ assuming the *null hypothesis* that the data come from a normal distribution. A small p-value indicates that something unusual happens if the null hypothesis is true, and thus we consider the null hypothesis unlikely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro.test(lmfit.res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p-value is very small, we reject the null hypothesis in favour of the alternative hypothesis that the data do not come from a normal distribution. This is in line with our Q-Q plot suspicion. What this implies is that strictly speaking, any further statistical test should be taken with a grain of salt. At the same time, note that the violation of the normality assumption appears to be mild (subjectively), and we may still proceed to take the model seriously for further analysis.\n",
    "\n",
    "**Goodness of fit.**\n",
    "Now, let's return to the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(lmfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the p-values for both intercept and fheight are extremely small, this suggests that they are unlikely to be equal to zero.  \n",
    "\n",
    "|<center>TASK 3</center>|\n",
    "| ---- |\n",
    "| State TRUE/FALSE for the following question. <br> R-square decreases as sum of square errors increases (with constant sum of square regression) <br> R-square is close to 0 if there is no relationship between dependent and independent variable in simple linear regression <br> R-square is zero (or undefined) if regression equation contains only the intercept term. <br> Store the boolean answers inside \"boolean1\" vector in the same sequence as that of questions. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here]\n",
    "boolean1 <- c(FALSE, TRUE, TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uncertainty in model parameters.**\n",
    "The estimated parameters are random, because they are computed from a random sample from a simple linear model. If the data is indeed generated by a simple linear model, then when the sample size is large, the estimated parameters will be close to the true parameters. The confidence interval (CI) is often used as a way to describe how accurate the estimates are. A CI with confidence level $p$ is an interval that has probability $p$ of containing the true value. Be very careful when interpreting this statement: here the true value is unknown but fixed, and the CI is random (due to randomness in the sample). The statement thus means that if we use the same procedure to construct a level $p$ CI on many many samples, a fraction $p$ of the intervals will contain the true value.\n",
    "\n",
    "In R, the `confint` function can be used to obtain confidence intervals. By default, a 95% confidence interval is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confint(lmfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus a 95% CI for the intercept is [30.2963477, 37.489253], and a 95% CI for fheight is [0.4609158, 0.567096]. It should be emphasized that once the CIs have been calculated, the true parameters are either in the intervals or not. What we can say is that a larger CI indicates higher uncertainty in the estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predition.**\n",
    "We can use the fitted linear model to make predictions on data. Such predictions are uncertain as well, and \n",
    "we can construct confidence intervals for the predictions to quantify our degree of uncertainty in the prediction. If the estimated linear equation is $y = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x$, then given any $x$, we can simply use the RHS as a point estimate. This can be done using the `predict` function as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(lmfit, data.frame(fheight=67))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you want to read the documentation of the `predict` function, you need to type `help(predict.lm)` in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 4</center>|\n",
    "| ---- |\n",
    "| Modify `return` statement in the below function such that it predicts the son's height (using co-efficients of lmfit model built previously) when father's height is passed as argument. <br> The answer should be rounded to two decimal places. <br> DO NOT change anything other than the equation in the `return` statement . |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>(Intercept):</strong> 97.12"
      ],
      "text/latex": [
       "\\textbf{(Intercept):} 97.12"
      ],
      "text/markdown": [
       "**(Intercept):** 97.12"
      ],
      "text/plain": [
       "(Intercept) \n",
       "      97.12 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelFun = function(x) {\n",
    "    predict = 5 ;\n",
    "    return(round(coef(lmfit)[1]+(coef(lmfit)[2]*x), 2))\n",
    "}\n",
    "\n",
    "equa1 = modelFun(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point prediction above is generally different from the true value due to two sources of randomness: (a) the estimated parameters are random and only approximate; (b) the simple linear model has an error term. \n",
    "\n",
    "When we consider the uncertainty in the prediction, we may only consider (a), or consider both (a) and (b). If we consider only (a), we can construct a confidence interval for $\\hat{\\beta}_{0} + \\hat{\\beta}_{1} x$ - while the distribution of this prediction has a very complex form, we can again use the `predict` function to help us to obtain a CI. A 95% CI for sheight when fheight=67 in is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(lmfit,data.frame(fheight=67),interval=\"confidence\",level = 0.95,type=\"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's construct a plot of the data, the fitted regression line, and a pointwise 95% confidence interval for the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newfheight<-seq(min(fheight$fheight)-1, max(fheight$fheight)+1, by=0.1)\n",
    "lmfitci<-predict(lmfit,data.frame(fheight=newfheight),interval=\"confidence\",level = 0.95,type=\"response\")\n",
    "plot(fs_height$fheight,fs_height$sheight,main=\"Father and Son Heights\", xlab=\"Father Heights (in)\",ylab=\"Son Heights (in)\",col=rgb(1,0.3,0.1,0.5))\n",
    "lines(newfheight,lmfitci[,1],col=\"black\",lty=1)\n",
    "lines(newfheight,lmfitci[,2],col=\"black\",lty=2)\n",
    "lines(newfheight,lmfitci[,3],col=\"black\",lty=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently, we are often interested to take both the uncertainty in the parameter estimates and the uncertainty due to the error term into account, and produce an interval that captures both forms of uncertainty. This would give us the typical range of responses be for a particular predictor value.\n",
    "\n",
    "Such interval is called a *prediction interval*.  Let's construct a 95% prediction interval for a predictor of 67 in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(lmfit,data.frame(fheight=67),interval=\"predict\",level=0.95,type=\"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice immediately that this interval is significantly wider than the confidence interval we computed earlier.\n",
    "\n",
    "|<center>TASK 5</center>|\n",
    "| ---- |\n",
    "| Find pointwise 95% prediction interval of the regression line for the father's heights (newfheight) given below. <br> Store the answer inside \"pInt\" matrix. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 19 × 2 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>lwr</th><th scope=col>upr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>58.89134</td><td>68.51895</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>59.41073</td><td>69.02757</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>59.92953</td><td>69.53678</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>60.44776</td><td>70.04656</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>60.96540</td><td>70.55694</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>61.48245</td><td>71.06789</td></tr>\n",
       "\t<tr><th scope=row>7</th><td>61.99892</td><td>71.57943</td></tr>\n",
       "\t<tr><th scope=row>8</th><td>62.51480</td><td>72.09157</td></tr>\n",
       "\t<tr><th scope=row>9</th><td>63.03010</td><td>72.60428</td></tr>\n",
       "\t<tr><th scope=row>10</th><td>63.54480</td><td>73.11759</td></tr>\n",
       "\t<tr><th scope=row>11</th><td>64.05892</td><td>73.63149</td></tr>\n",
       "\t<tr><th scope=row>12</th><td>64.57245</td><td>74.14597</td></tr>\n",
       "\t<tr><th scope=row>13</th><td>65.08538</td><td>74.66104</td></tr>\n",
       "\t<tr><th scope=row>14</th><td>65.59773</td><td>75.17671</td></tr>\n",
       "\t<tr><th scope=row>15</th><td>66.10950</td><td>75.69296</td></tr>\n",
       "\t<tr><th scope=row>16</th><td>66.62067</td><td>76.20979</td></tr>\n",
       "\t<tr><th scope=row>17</th><td>67.13126</td><td>76.72721</td></tr>\n",
       "\t<tr><th scope=row>18</th><td>67.64127</td><td>77.24522</td></tr>\n",
       "\t<tr><th scope=row>19</th><td>68.15069</td><td>77.76381</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 19 × 2 of type dbl\n",
       "\\begin{tabular}{r|ll}\n",
       "  & lwr & upr\\\\\n",
       "\\hline\n",
       "\t1 & 58.89134 & 68.51895\\\\\n",
       "\t2 & 59.41073 & 69.02757\\\\\n",
       "\t3 & 59.92953 & 69.53678\\\\\n",
       "\t4 & 60.44776 & 70.04656\\\\\n",
       "\t5 & 60.96540 & 70.55694\\\\\n",
       "\t6 & 61.48245 & 71.06789\\\\\n",
       "\t7 & 61.99892 & 71.57943\\\\\n",
       "\t8 & 62.51480 & 72.09157\\\\\n",
       "\t9 & 63.03010 & 72.60428\\\\\n",
       "\t10 & 63.54480 & 73.11759\\\\\n",
       "\t11 & 64.05892 & 73.63149\\\\\n",
       "\t12 & 64.57245 & 74.14597\\\\\n",
       "\t13 & 65.08538 & 74.66104\\\\\n",
       "\t14 & 65.59773 & 75.17671\\\\\n",
       "\t15 & 66.10950 & 75.69296\\\\\n",
       "\t16 & 66.62067 & 76.20979\\\\\n",
       "\t17 & 67.13126 & 76.72721\\\\\n",
       "\t18 & 67.64127 & 77.24522\\\\\n",
       "\t19 & 68.15069 & 77.76381\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 19 × 2 of type dbl\n",
       "\n",
       "| <!--/--> | lwr | upr |\n",
       "|---|---|---|\n",
       "| 1 | 58.89134 | 68.51895 |\n",
       "| 2 | 59.41073 | 69.02757 |\n",
       "| 3 | 59.92953 | 69.53678 |\n",
       "| 4 | 60.44776 | 70.04656 |\n",
       "| 5 | 60.96540 | 70.55694 |\n",
       "| 6 | 61.48245 | 71.06789 |\n",
       "| 7 | 61.99892 | 71.57943 |\n",
       "| 8 | 62.51480 | 72.09157 |\n",
       "| 9 | 63.03010 | 72.60428 |\n",
       "| 10 | 63.54480 | 73.11759 |\n",
       "| 11 | 64.05892 | 73.63149 |\n",
       "| 12 | 64.57245 | 74.14597 |\n",
       "| 13 | 65.08538 | 74.66104 |\n",
       "| 14 | 65.59773 | 75.17671 |\n",
       "| 15 | 66.10950 | 75.69296 |\n",
       "| 16 | 66.62067 | 76.20979 |\n",
       "| 17 | 67.13126 | 76.72721 |\n",
       "| 18 | 67.64127 | 77.24522 |\n",
       "| 19 | 68.15069 | 77.76381 |\n",
       "\n"
      ],
      "text/plain": [
       "   lwr      upr     \n",
       "1  58.89134 68.51895\n",
       "2  59.41073 69.02757\n",
       "3  59.92953 69.53678\n",
       "4  60.44776 70.04656\n",
       "5  60.96540 70.55694\n",
       "6  61.48245 71.06789\n",
       "7  61.99892 71.57943\n",
       "8  62.51480 72.09157\n",
       "9  63.03010 72.60428\n",
       "10 63.54480 73.11759\n",
       "11 64.05892 73.63149\n",
       "12 64.57245 74.14597\n",
       "13 65.08538 74.66104\n",
       "14 65.59773 75.17671\n",
       "15 66.10950 75.69296\n",
       "16 66.62067 76.20979\n",
       "17 67.13126 76.72721\n",
       "18 67.64127 77.24522\n",
       "19 68.15069 77.76381"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Place your Answer here]\n",
    "newfheight<-seq(58,76)\n",
    "lmfitci <- predict(lmfit,data.frame(fheight=newfheight),interval=\"prediction\")\n",
    "pInt <- lmfitci[, c(\"lwr\", \"upr\")]\n",
    "pInt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 6</center>|\n",
    "| ---- |\n",
    "| For the (fheight, sheight) pairs in the dataset, calculate the proportion of sheight that fall into the 95% prediction interval for fheight. <br> Store the answer inside \"prop1\" variable, rounded to 2 decimal places using `round` statement. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in fs_height$sheight >= pInt[, \"lwr\"]:\n",
      "“longer object length is not a multiple of shorter object length”\n",
      "Warning message in fs_height$sheight <= pInt[, \"upr\"]:\n",
      "“longer object length is not a multiple of shorter object length”\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.77"
      ],
      "text/latex": [
       "0.77"
      ],
      "text/markdown": [
       "0.77"
      ],
      "text/plain": [
       "[1] 0.77"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Place your Answer here]\n",
    "\n",
    "interval <-  fs_height[fs_height$fheight >= 58 & fs_height$fheight <= 76 & fs_height$sheight >= pInt[, \"lwr\"] & fs_height$sheight <= pInt[, \"upr\"],]\n",
    "prop1 <- round(nrow(interval)/nrow(fs_height),2)\n",
    "prop1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the above analysis has been carried out on the training set only. To evaluate whether the simple linear model can possibly generalize to new data, we need to train it on a training set, and evaluate it on a test set.\n",
    "Let's first construct a 70/30 train-test split using Pearson's dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = length(fheight$fheight)\n",
    "set.seed(107)\n",
    "trainidx = sample(1:N, floor(0.7*N), replace=FALSE)\n",
    "tr = data.frame(fheight=fheight$fheight[trainidx], sheight=sheight$sheight[trainidx])\n",
    "ts = data.frame(fheight=fheight$fheight[-trainidx], sheight=sheight$sheight[-trainidx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 7</center>|\n",
    "| ---- |\n",
    "| Train a simple linear model using the training set (fheight_tr, sheight_tr). <br> Store the MSEs of the model on the training set (fheight_tr, sheight_tr) and the test set (fheight_ts, sheight_ts) in \"mseTrain1\" and \"mseTest1\" variables respectively, rounded to 2 decimal places using `round` function. <br> Compare the MSEs and comment on the model's ability to generalize to new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "5.89"
      ],
      "text/latex": [
       "5.89"
      ],
      "text/markdown": [
       "5.89"
      ],
      "text/plain": [
       "[1] 5.89"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "6.06"
      ],
      "text/latex": [
       "6.06"
      ],
      "text/markdown": [
       "6.06"
      ],
      "text/plain": [
       "[1] 6.06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Place your Answer here].  Include the desciptive portion of the answer as an R comment.\n",
    "pred <- predict(lmfit,data.frame(fs_height),interval=\"prediction\")\n",
    "model <- lm(sheight ~ fheight, data = tr)\n",
    "train <- predict(model, newdata = tr)\n",
    "test <- predict(model, newdata = ts)\n",
    "mseTrain <- round(mean((tr$sheight - train)^2), 2)\n",
    "mseTest <- round(mean((ts$sheight - test)^2), 2)\n",
    "mseTrain\n",
    "mseTest\n",
    "# test is lower then train, it means this model can not make a accurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Let's return to the HR analytics data from Prac. 3.\n",
    "\n",
    "To think about trying to predict the likelihood of our best and most experienced employees leaving based on the average monthly hours worked.\n",
    "\n",
    "To that end, we'll load up the data and subset out the best (evaluation of 0.8 or higher) and the most experienced employees (with the company for 4 or more years).  We'll also create a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"readr\")\n",
    "HR_comma_sep <- read_csv(\"HR_comma_sep.csv\")\n",
    "HR_best <- HR_comma_sep[(HR_comma_sep$last_evaluation>=0.8)&(HR_comma_sep$time_spend_company>=4),]\n",
    "head(HR_best,10)\n",
    "summary(HR_best)\n",
    "set.seed(8888) # Set a seed for the random number generator;\n",
    "trainidx<-sample(nrow(HR_best), floor(nrow(HR_best) * 0.5))\n",
    "HR_best_train<-HR_best[trainidx,]\n",
    "HR_best_test<-HR_best[-trainidx,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll plot `left` vs `Average Monthly Hours` in our training set as a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(HR_best_train$average_montly_hours,HR_best_train$left,main=\"Left vs Average Monthly Hours Worked (Best)\", xlab=\"Average Monthly Hours\",ylab=\"Left\",col=rgb(0,0,1,0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from this plot that the more hours they work on average, the more likely that the best and most experienced employees will leave.\n",
    "\n",
    "Let's fit a logistic regression model to the data, and plot the resulting regression curve. This can be done using the built-in `glm` function, with the `family` argument set to `binomial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfit<-glm(left~average_montly_hours,data=HR_best_train,family=binomial)\n",
    "summary(logfit)\n",
    "newamh<-seq(96,310)\n",
    "predprobs<-predict(logfit,data.frame(average_montly_hours=newamh),type=\"response\")\n",
    "plot(HR_best_train$average_montly_hours,HR_best_train$left,main=\"Left vs Average Monthly Hours Worked (Best)\", xlab=\"Average Monthly Hours\",ylab=\"Left\",col=rgb(0,0,1,0.5))\n",
    "lines(newamh,predprobs,col=\"black\",lty=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 8</center>|\n",
    "| ---- |\n",
    "| For every 1 hour increase in average monthly hours worked, the log odds of leaving increase by how much ? <br> Store the answer inside \"logOdds1\" variable. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>average_montly_hours:</strong> 0.0381597622213795"
      ],
      "text/latex": [
       "\\textbf{average\\textbackslash{}\\_montly\\textbackslash{}\\_hours:} 0.0381597622213795"
      ],
      "text/markdown": [
       "**average_montly_hours:** 0.0381597622213795"
      ],
      "text/plain": [
       "average_montly_hours \n",
       "          0.03815976 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Place your Answer here]\n",
    "logfit <- glm(left ~ average_montly_hours, data = HR_best_train, family = binomial)\n",
    "logOdds1 <- coef(logfit)[\"average_montly_hours\"]\n",
    "logOdds1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "We can use a threshold together with the regression curve to create two groups.  We can also observe the training error and test error we make.\n",
    "\n",
    "Let's start with a threshold of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HR_best_train$probs<-predict(logfit,data.frame(average_montly_hours=HR_best_train$average_montly_hours),type=\"response\")\n",
    "HR_best_train$predleft<-as.integer(HR_best_train$probs>=0.5)\n",
    "table(HR_best_train$predleft,HR_best_train$left)\n",
    "\n",
    "HR_best_test$probs<-predict(logfit,data.frame(average_montly_hours=HR_best_test$average_montly_hours),type=\"response\")\n",
    "HR_best_test$predleft<-as.integer(HR_best_test$probs>=0.5)\n",
    "table(HR_best_test$predleft,HR_best_test$left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 9</center>|\n",
    "| ---- |\n",
    "| What are the (1) error rate, (2) sensitivity, (3) specificity, and (4) precision in Training and Testing for this classifier? <br> Store the answer inside \"errorRateTrain1\", \"errorRateTest1\", \"sensitivityTrain1\", \"sensitivityTest1\", \"specificityTrain1\", \"specificityTest1\", \"precisionTrain1\", \"precisionTest1\". |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.201369863013699"
      ],
      "text/latex": [
       "0.201369863013699"
      ],
      "text/markdown": [
       "0.201369863013699"
      ],
      "text/plain": [
       "[1] 0.2013699"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.904651162790698"
      ],
      "text/latex": [
       "0.904651162790698"
      ],
      "text/markdown": [
       "0.904651162790698"
      ],
      "text/plain": [
       "[1] 0.9046512"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.646666666666667"
      ],
      "text/latex": [
       "0.646666666666667"
      ],
      "text/markdown": [
       "0.646666666666667"
      ],
      "text/plain": [
       "[1] 0.6466667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.785858585858586"
      ],
      "text/latex": [
       "0.785858585858586"
      ],
      "text/markdown": [
       "0.785858585858586"
      ],
      "text/plain": [
       "[1] 0.7858586"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.204654346338125"
      ],
      "text/latex": [
       "0.204654346338125"
      ],
      "text/markdown": [
       "0.204654346338125"
      ],
      "text/plain": [
       "[1] 0.2046543"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.921146953405018"
      ],
      "text/latex": [
       "0.921146953405018"
      ],
      "text/markdown": [
       "0.921146953405018"
      ],
      "text/plain": [
       "[1] 0.921147"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.626602564102564"
      ],
      "text/latex": [
       "0.626602564102564"
      ],
      "text/markdown": [
       "0.626602564102564"
      ],
      "text/plain": [
       "[1] 0.6266026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.76792828685259"
      ],
      "text/latex": [
       "0.76792828685259"
      ],
      "text/markdown": [
       "0.76792828685259"
      ],
      "text/plain": [
       "[1] 0.7679283"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Place your Answer here]\n",
    "errorRateTrain1 <- mean(HR_best_train$predleft != HR_best_train$left)\n",
    "sensitivityTrain1 <- sum(HR_best_train$predleft == 1 & HR_best_train$left == 1)/sum(HR_best_train$left == 1)\n",
    "specificityTrain1 <- sum(HR_best_train$predleft == 0 & HR_best_train$left == 0)/sum(HR_best_train$left == 0)\n",
    "precisionTrain1 <- sum(HR_best_train$predleft == 1 & HR_best_train$left == 1)/sum(HR_best_train$predleft == 1)\n",
    "errorRateTest1 <- mean(HR_best_test$predleft != HR_best_test$left)\n",
    "sensitivityTest1 <- sum(HR_best_test$predleft == 1 & HR_best_test$left == 1)/sum(HR_best_test$left == 1)\n",
    "specificityTest1 <- sum(HR_best_test$predleft == 0 & HR_best_test$left == 0)/sum(HR_best_test$left == 0)\n",
    "precisionTest1 <- sum(HR_best_test$predleft == 1 & HR_best_test$left == 1)/sum(HR_best_test$predleft == 1)\n",
    "errorRateTrain1\n",
    "sensitivityTrain1\n",
    "specificityTrain1\n",
    "precisionTrain1\n",
    "errorRateTest1\n",
    "sensitivityTest1\n",
    "specificityTest1\n",
    "precisionTest1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 10</center>|\n",
    "| ---- |\n",
    "| Perform two fold cross-validation (CV) of the logistic regression classifier above on the training set. <br> Store the cross-validation error (the average of the error rates on the two folds) in \"cv1\" variable. Round \"cv1\" to 1 decimal place using `round` fn.<br> Set the seed of sampling to 55. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.2"
      ],
      "text/latex": [
       "0.2"
      ],
      "text/markdown": [
       "0.2"
      ],
      "text/plain": [
       "[1] 0.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Place your Answer here]\n",
    "library(boot)\n",
    "set.seed(55)\n",
    "n <- nrow(HR_best_train)\n",
    "folds <- sample(rep(1:2, each = n %/% 2), n)\n",
    "errors <- numeric(2)\n",
    "for (i in 1:2) {\n",
    "  train <- HR_best_train[folds != i, ]\n",
    "  valid <- HR_best_train[folds == i, ]\n",
    "  logfit <- glm(left ~ average_montly_hours, data = train, family = \"binomial\")\n",
    "  pred <- predict(logfit, newdata = valid, type = \"response\")\n",
    "  pred <- as.integer(pred >= 0.5)\n",
    "  errors[i] <- sum(pred != valid$left) / nrow(valid)\n",
    "}\n",
    "cv1 <- mean(errors)\n",
    "cv1 <- round(cv1, 1)\n",
    "cv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantities we computed above were just for a single choice of threshold.  We can gauge the tradeoff between true positive rates (sensitivity) and false positive rates (1-specificity) by trying all possible thresholds. This allows us to generate a plot of true positive rate against the false positive rate - a curve known the ROC (receiver operating characteristic) curve.\n",
    "\n",
    "We'll do this now for both the training data, as well as computing the AUC (Area Under the Curve).\n",
    "**Note:** We will need to install the \"ROCR\" package. Please see the following Ed discussion if you need help installing this package outside of the Jupyter environment: https://edstem.org/au/courses/10549/discussion/1271472\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"ROCR\")\n",
    "library(\"ROCR\")\n",
    "trainROC<-performance(prediction(HR_best_train$probs,HR_best_train$left),\"tpr\",\"fpr\")\n",
    "plot(trainROC)\n",
    "abline(a=0, b= 1,lty=2)\n",
    "trainAUC<-as.double(performance(prediction(HR_best_train$probs,HR_best_train$left),\"auc\")@y.values)\n",
    "trainAUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 11</center>|\n",
    "| ---- |\n",
    "| Construct the ROC curve for the test data, and compute the AUC and store the same in \"auc1\" (rounded to 4 decimal places). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.8351"
      ],
      "text/latex": [
       "0.8351"
      ],
      "text/markdown": [
       "0.8351"
      ],
      "text/plain": [
       "[1] 0.8351"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAADAFBMVEUAAAABAQECAgIDAwME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\nkAA4QEgAHCAkAA4QEgAH/w867kwEWsOcvQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Place your Answer here]\n",
    "testROC<-performance(prediction(HR_best_test$probs, HR_best_test$left),\"tpr\",\"fpr\")\n",
    "plot(testROC)\n",
    "abline(a = 0, b = 1, lty = 2)\n",
    "auc1 <- as.numeric(performance(prediction(HR_best_test$probs, HR_best_test$left), \"auc\")@y.values)\n",
    "auc1 <- round(auc1, 4)\n",
    "auc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we have been ignoring all of the other predictors so far.\n",
    "\n",
    "It is simple enough to modify the logistic regression to include others we have identified as important from Prac. 3.\n",
    "\n",
    "Let's include satisfaction level, number of projects, and their interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfit2<-glm(left~average_montly_hours*satisfaction_level*number_project,data=HR_best_train,family=binomial)\n",
    "summary(logfit2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst the interpretation of the estimated coefficients becomes trickier for this model, for this practical we're only interested in its classification performance.\n",
    "\n",
    "Let's evaluate the training performance of the classifier associated with this more complicated logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HR_best_train$probs2<-predict(logfit2,data.frame(average_montly_hours=HR_best_train$average_montly_hours,satisfaction_level=HR_best_train$satisfaction_level,number_project=HR_best_train$number_project),type=\"response\")\n",
    "trainROC2<-performance(prediction(HR_best_train$probs2,HR_best_train$left),\"tpr\",\"fpr\")\n",
    "plot(trainROC2)\n",
    "abline(a=0, b= 1,lty=2)\n",
    "trainAUC2<-as.double(performance(prediction(HR_best_train$probs2,HR_best_train$left),\"auc\")@y.values)\n",
    "trainAUC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the model fitted using more predictors produces a better classifier for the training data.\n",
    "\n",
    "|<center>TASK 12</center>|\n",
    "| ---- |\n",
    "| Construct the ROC curve for the test data and store the AUC for the new logistic regression model in \"auc2\" (rounded to 4 decimal places). What do you observe?  Comment on one potential pitfall associated with more complex models. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.9136"
      ],
      "text/latex": [
       "0.9136"
      ],
      "text/markdown": [
       "0.9136"
      ],
      "text/plain": [
       "[1] 0.9136"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAADAFBMVEUAAAABAQECAgIDAwME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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Place your Answer here].  Include the desciptive portion of the answer as an R comment.\n",
    "logfit2<-glm(left~average_montly_hours*satisfaction_level*number_project,data=HR_best_test,family=binomial)\n",
    "HR_best_test$probs2<-predict(logfit2,data.frame(average_montly_hours=HR_best_test$average_montly_hours,satisfaction_level=HR_best_test$satisfaction_level,number_project=HR_best_test$number_project),type=\"response\")\n",
    "testROC2<-performance(prediction(HR_best_test$probs2,HR_best_test$left),\"tpr\",\"fpr\")\n",
    "plot(testROC2)\n",
    "abline(a=0, b= 1,lty=2)\n",
    "auc2<-as.double(performance(prediction(HR_best_test$probs2,HR_best_test$left),\"auc\")@y.values)\n",
    "auc2 <- round(auc2, 4)\n",
    "auc2\n",
    "#the more complex model with additional predictors has better predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model selection.** Next task explores CV's ability to select a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 13</center>|\n",
    "| ---- |\n",
    "| Calculate the 2-fold CV error of the more complex logistic regression classifier on the training set. Use the same two folds as for the old classifier, and use 0.5 as the threshold too. <br> Store the error inside \"cv2\" variable. Round \"cv2\" to 1 decimal place using `round` function. <br>Does two fold CV help to find the classifier with better test set performance? Explain your answer.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.1"
      ],
      "text/latex": [
       "0.1"
      ],
      "text/markdown": [
       "0.1"
      ],
      "text/plain": [
       "[1] 0.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Place your Answer here]\n",
    "set.seed(55)\n",
    "HR_best_train$fold <- factor(rep(1:2, length.out = nrow(HR_best_train)))\n",
    "logfit2_fold1 <- glm(left ~ average_montly_hours*satisfaction_level*number_project, data = HR_best_train[HR_best_train$fold == 1,], family = binomial)\n",
    "probs2_fold2 <- predict(logfit2_fold1, data.frame(average_montly_hours = HR_best_train[HR_best_train$fold == 2,]$average_montly_hours, satisfaction_level = HR_best_train[HR_best_train$fold == 2,]$satisfaction_level, number_project = HR_best_train[HR_best_train$fold == 2,]$number_project), type = \"response\")\n",
    "class2_fold2 <- ifelse(probs2_fold2 > 0.5, 1, 0)\n",
    "error2_fold2 <- mean(class2_fold2 != HR_best_train[HR_best_train$fold == 2,]$left)\n",
    "logfit2_fold2 <- glm(left ~ average_montly_hours*satisfaction_level*number_project, data = HR_best_train[HR_best_train$fold == 2,], family = binomial)\n",
    "probs2_fold1 <- predict(logfit2_fold2, data.frame(average_montly_hours = HR_best_train[HR_best_train$fold == 1,]$average_montly_hours, satisfaction_level = HR_best_train[HR_best_train$fold == 1,]$satisfaction_level, number_project = HR_best_train[HR_best_train$fold == 1,]$number_project), type = \"response\")\n",
    "class2_fold1 <- ifelse(probs2_fold1 > 0.5, 1, 0)\n",
    "error2_fold1 <- mean(class2_fold1 != HR_best_train[HR_best_train$fold == 1,]$left)\n",
    "cv2 <- mean(c(error2_fold1, error2_fold2))\n",
    "cv2 <- round(cv2, 1)\n",
    "cv2\n",
    "#The results showed that the more complex classifier did not result in a significant improvement in performance compared to the simpler one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Classification Techniques\n",
    "\n",
    "It is relatively easy to carry out classification using other supervised learning techniques.\n",
    "\n",
    "There are very well-written implementations of many other algorithms not discussed in this course, such as LDA (linear discriminative anaysis), QDA (quadratic discriminative analysis). Training and prediction using these models can generally be done using functions with similar structure as in previous examples, except that there may be algorithm specific parameters that we need to specify.\n",
    "\n",
    "We illustrate how to construct an LDA-based classifier using the three predictors and their interactions we had before, and evaluate it on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"MASS\")\n",
    "ldafit<-lda(left~average_montly_hours*satisfaction_level*number_project,data=HR_best_train)\n",
    "temp<-predict(ldafit,data.frame(average_montly_hours=HR_best_test$average_montly_hours,satisfaction_level=HR_best_test$satisfaction_level,number_project=HR_best_test$number_project),type=\"response\")\n",
    "HR_best_test$probs3<-temp$posterior[,2]\n",
    "testROC3<-performance(prediction(HR_best_test$probs3,HR_best_test$left),\"tpr\",\"fpr\")\n",
    "plot(testROC3)\n",
    "abline(a=0, b= 1,lty=2)\n",
    "testAUC3<-as.double(performance(prediction(HR_best_test$probs3,HR_best_test$left),\"auc\")@y.values)\n",
    "testAUC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to linear discriminant analysis, the `MASS` package has support for quadratic discriminant analysis via the command `qda`.  The `class` package has support for K nearest neighbours via `knn` and the `e1071` package has support for support vector machines via `svm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "As a final exercise, let's subset all of our employee data again (not only best) to only include those that have left.\n",
    "\n",
    "In Prac. 3, we visually identified several *clusters* of leavers.  We will now use the k-means clustering algorithm to capture these clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HR_left<-HR_comma_sep[HR_comma_sep$left==1,]\n",
    "head(HR_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's focus on three variables: average monthly hours, satisfaction level, and last evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"ggplot2\") # Expanded plotting functionality over \"lattice\" package\n",
    "x<-cbind(HR_left$average_montly_hours,HR_left$satisfaction_level,HR_left$last_evaluation)\n",
    "kmfit<-kmeans(x,3,nstart=25) # Find the best 3 clusters using 25 random sets of (distinct) rows in x as initial centres.\n",
    "pairs(x,col=(kmfit$cluster),labels=c(\"Av. Mon. Hrs\",\"Sat. Lvl\",\"Last Eval.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing you will notice is that the three clusters that are found are not what we would expect.\n",
    "\n",
    "This is because the variables are on different scales; since average monthly hours takes on values so much larger than  satisfaction level and last evaluation, the algorithm effectively prioritises just this variable.\n",
    "\n",
    "A quick remedy to this problem is achieved by adjusting the scale of satisfaction level and last evaluation by a factor of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x<-cbind(HR_left$average_montly_hours,100*HR_left$satisfaction_level,100*HR_left$last_evaluation)\n",
    "set.seed(55)\n",
    "kmfit<-kmeans(x,3,nstart=25) # Find the best 3 clusters using 25 random restarts\n",
    "pairs(x,col=(kmfit$cluster),labels=c(\"Av. Mon. Hrs\",\"Sat. Lvl\",\"Last Eval.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<center>TASK 14</center>|\n",
    "| ---- |\n",
    "| Which cluster (index) has most data points. Store the answer inside \"cluster1\" variable. <br> Partition the dataset into the three clusters we have just identified.  Use quantitative and visual EDA to explore the typical characteristics of each cluster.  Based on this, what is a possible explanation for the clusters? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Place your Answer here].  Include the desciptive portion of the answer as an R comment.\n",
    "cluster1 <- which.max(table(kmfit$cluster))\n",
    "cluster1\n",
    "library(\"ggplot2\")\n",
    "ggplot(data=data.frame(x, cluster=kmfit$cluster), aes(x=x[,1], y=x[,2], color=factor(cluster))) +\n",
    "  geom_point() +\n",
    "  labs(title=\"Cluster Plot\") +\n",
    "  xlab(\"Average Monthly Hours\") +\n",
    "  ylab(\"Satisfaction Level\")\n",
    "# we can find the left people have larger average monthly hours for working and satusfaction level is not too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This Line gets printed if there is no error, when Kernel -> Restart & Run All\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
